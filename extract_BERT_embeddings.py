# -*- coding: utf-8 -*-
"""GPT2_features_extract.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZoqLKnCTDYnkLAy2MSTdqn1e3o-aN6LL

================== IMPORT LIBRARIES ==================
"""

# Summary: using to extract BERT in general, input id_column, text_column, header 
# Input: question raw dataset
# Output: .json file with each record is "id", "BERT_all_words"   

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import argparse
import collections
import logging
import json
import csv 
import numpy as np
csv.field_size_limit(5000000)
import time, datetime
from tqdm import tqdm

import torch
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler
from torch.utils.data.distributed import DistributedSampler

from transformers import BertConfig, BertModel, BertTokenizer
MODEL_CLASSES = {'bert': (BertConfig, BertModel, BertTokenizer)}

logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', 
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger(__name__)

"""================== METHODS DEFINITIONS =================="""

class InputSample(object):

    def __init__(self, message_id, text):
        self.message_id = message_id
        self.text = text


class InputFeatures(object):
    """A single set of features of data."""

    def __init__(self, message_id, tokens, text, input_ids, attention_mask):
        self.message_id = message_id
        self.tokens = tokens
        self.text = text
        self.input_ids = input_ids
        self.attention_mask = attention_mask


def convert_examples_to_features(samples, seq_length, tokenizer):
    """Loads a data file into a list of `InputBatch`s."""

    features = []
    for sample in samples:
        message_id = sample.message_id
        text = sample.text
        
        # tokenize text
        tokens = tokenizer.tokenize(text)
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        # padding and truncating      
        if (len(tokens) > seq_length):
            tokens = tokens[:seq_length]
            tokens[-1] = '[SEP]'
            attention_mask = [1]*seq_length  
        else:
            padded_tokens = tokens + ['[PAD]']*(seq_length - len(tokens))
            attention_mask = [1]*len(tokens) + [0]*(seq_length - len(tokens)) 
            tokens = padded_tokens    
        
        # convert to tokens ids        
        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        # add to features list
        features.append(
            InputFeatures(
                message_id = message_id,    
                text = text,
                tokens=tokens,
                input_ids=input_ids,
                attention_mask = attention_mask))        
                   
        # print examples
        if len(features) < 6:
            logger.info("*** Example ***")
            logger.info("message_id: %s" % (message_id))
            logger.info("text: %s" % (text))
            logger.info("tokens: %s" % " ".join([str(x) for x in tokens]))
            logger.info("tokens_convert_back: %s" % " ".join([str(x) for x in tokenizer.convert_ids_to_tokens(input_ids)]))
            logger.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
            logger.info("attention_mask: %s" % " ".join([str(x) for x in attention_mask]))
        
    return features


def read_examples(input_file, id_column, text_column, header):

    with open(input_file, "r", encoding='utf-8') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')        
        # skip header 
        if header:
            next(csv_reader)
            
        # loop through all rows
        samples = []
        count = 0
        for row in csv_reader:
            text = row[text_column]
            message_id = row[id_column]
            samples.append(InputSample(message_id, text))
            count += 1
        print("Number of records: "+str(count))      

        return samples

"""================== MAIN =================="""

def main():
    start = time.time()
    
    parser = argparse.ArgumentParser()

    ## Required parameters
    parser.add_argument("--input_file", default=None, type=str, required=True)
    parser.add_argument("--output_file", default=None, type=str, required=True)
    parser.add_argument("--model_type", default="bert", type=str,
                        help="The model architecture to be fine-tuned.")
    parser.add_argument("--model_name_or_path", default="bert-base-cased", type=str,
                        help="The model checkpoint for weights initialization.")
    parser.add_argument("--config_name", default="", type=str,
                        help="Optional pretrained config name or path if not the same as model_name_or_path")
    parser.add_argument("--tokenizer_name", default="", type=str,
                        help="Optional pretrained tokenizer name or path if not the same as model_name_or_path")
    parser.add_argument("--cache_dir", default="", type=str,
                        help="Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)")
    parser.add_argument("--block_size", default=-1, type=int,
                        help="Optional input sequence length after tokenization."
                             "The training dataset will be truncated in block of this size for training."
                             "Default to the model max input length for single sentence inputs (take into account special tokens).")
    
    ## Other parameters
    parser.add_argument("--do_lower_case", action='store_true', help="Set this flag if you are using an uncased model.")
    parser.add_argument("--layers", default="-1 -2 -3 -4", type=str)
    parser.add_argument("--batch_size", default=32, type=int, help="Batch size for predictions.")
    parser.add_argument("--local_rank",
                        type=int,
                        default=-1,
                        help = "local_rank for distributed training on gpus")
    parser.add_argument("--no_cuda",
                        action='store_true',
                        help="Whether not to use CUDA when available")
    parser.add_argument("--id_column",
                        type=int,
                        help="ID column for message ID")    
    parser.add_argument("--text_column",
                        type=int,
                        help="Text column used to extract BERT")
    parser.add_argument("--header",
                        type=bool,
                        help="Remove header if True")
    parser.add_argument("--layers_aggregation", 
                        default=None, type=str, 
                        required=True)
    
    args = parser.parse_args()

    if args.local_rank == -1 or args.no_cuda:
        device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
        n_gpu = torch.cuda.device_count()
    else:
        device = torch.device("cuda", args.local_rank)
        n_gpu = 1
        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs
        torch.distributed.init_process_group(backend='nccl')
    logger.info("device: {} n_gpu: {} distributed training: {}".format(device, n_gpu, bool(args.local_rank != -1)))

    layer_indexes = [int(x) for x in args.layers.split(" ")]

    # Load pretrained model and tokenizer
    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)
    config.output_hidden_states = True
    config.output_attentions = True
    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)
    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)
    model.to(device)


    # reading input text
    samples = read_examples(args.input_file, args.id_column, args.text_column, args.header)
    features = convert_examples_to_features(samples = samples, seq_length = args.block_size, tokenizer=tokenizer)
    

    if args.local_rank != -1:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],
                                                          output_device=args.local_rank)
    elif n_gpu > 1:
        model = torch.nn.DataParallel(model)

    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)
    all_sample_indices = torch.arange(all_input_ids.size(0), dtype=torch.long)  # all_sample_indices is for matching index in a batch with index in the original "features" variable

    eval_data = TensorDataset(all_input_ids, all_attention_mask, all_sample_indices)
    if args.local_rank == -1:
        eval_sampler = SequentialSampler(eval_data)
    else:
        eval_sampler = DistributedSampler(eval_data)
    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.batch_size)

    logger.info('Start extracting BERT.')
    model.eval()
    with open(args.output_file, "w", encoding='utf-8') as writer:
        for input_ids, attention_mask, sample_indices in tqdm(eval_dataloader):
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            sample_indices = sample_indices   
            
            # forward model
            last_hidden_state, pooler_output, hidden_states, attentions = model(input_ids = input_ids, attention_mask = attention_mask)
            hidden_states = np.asarray([layer_hidden_states.cpu().detach().numpy() for layer_hidden_states in hidden_states])
            attention_mask = np.asarray(attention_mask.cpu().detach())
            
            # only take interested layers
            hidden_states = [hidden_states[layer_index] for layer_index in layer_indexes]
            hidden_states = np.swapaxes(hidden_states, 0, 1)    # swap dimension to be [batch_size, num_layers, seq_length, hidden_size]
            # DEBUGGING
            # logger.info("hidden_states.shape: " + str(hidden_states.shape))
            
            # compute embedding for each sample
            for b, sample_index in enumerate(sample_indices):

                # get encoder_past of one sample
                hidden_state_one_sample = hidden_states[b]  # this "b" is the order within a batch, "sample_index" is the order in the original "feautures" variable
                attention_mask_one_sample = attention_mask[b]  
                # logger.info("=====")
                # logger.info("hidden_state_one_sample.shape: " + str(hidden_state_one_sample.shape))  # [batch_size, num_layers, seq_length, hidden_size]  
                
                
                # compute embedding across tokens (average across non-masked tokens)
                num_non_masked_tokens = np.sum(attention_mask_one_sample)
                hidden_state_one_sample = hidden_state_one_sample[:, :num_non_masked_tokens, :] # only get the first num_non_masked_tokens non_masked_tokens
                embedding_sample_across_tokens = np.mean(hidden_state_one_sample, axis = 1)
                # logger.info("num_non_masked_tokens: " + str(num_non_masked_tokens))    
                # logger.info("hidden_state_one_sample.shape: " + str(hidden_state_one_sample.shape))    
                # logger.info("embedding_sample_across_tokens.shape: " + str(embedding_sample_across_tokens.shape))    


                # compute embedding across layers
                if args.layers_aggregation == 'mean':
                    embedding_sample_across_layers = np.mean(embedding_sample_across_tokens, axis = 0)                
                    logger.info("embedding_sample_across_layers.shape: " + str(embedding_sample_across_layers.shape))    
                elif args.layers_aggregation == 'concat':
                    embedding_sample_across_layers = np.concatenate(embedding_sample_across_tokens, axis = 0)
                    logger.info("embedding_sample_across_layers.shape: " + str(embedding_sample_across_layers.shape))    


                # get sentence information
                feature = features[sample_index.item()]  # map from index in batch to index in the original "feautures" variable
                output_json = collections.OrderedDict()
                output_json["message_id"] = feature.message_id
                output_json["text"] = feature.text
                output_json["vector"] = embedding_sample_across_layers.tolist() 
                writer.write(json.dumps(output_json) + "\n")
                
                
    print ("Took %s to extract BERT embeddings." % (datetime.timedelta(seconds=(time.time()-start))))  
           
if __name__ == "__main__":
    main()